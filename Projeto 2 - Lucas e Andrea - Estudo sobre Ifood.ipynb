{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ciência dos Dados - Estudo sobre Ifood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Lucas Hix\n",
    "\n",
    "Nome: Andrea Mindlin Tessler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador automático de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações\n",
    "\n",
    "import datetime\n",
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "* Conta - Lucas Hix: ***@Peng1104Oficial***\n",
    "* Conta - Andrea Tessler: ***@Andrea25519779***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de autenticação do twitter:\n",
    "\n",
    "def mkAPI(filename):\n",
    "    # leitura do arquivo no formato JSON\n",
    "    \n",
    "    with open(filename) as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Configurando a biblioteca. Não modificar\n",
    "    \n",
    "    auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "    auth.set_access_token(data['access_token'], data['access_token_secret'])\n",
    "    \n",
    "    # Cria um objeto de API\n",
    "    return tweepy.API(auth)\n",
    "\n",
    "# Conta @Peng1104Oficial\n",
    "api_lucas = mkAPI(\"lucas-auth.pass\")\n",
    "\n",
    "# Conta @Andrea25519779\n",
    "api_andrea = mkAPI(\"andrea-auth.pass\")\n",
    "\n",
    "apis = [api_lucas, api_andrea]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Etapas do projeto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções essenciais para o projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variaveis de suporte para a captura de Tweets\n",
    "ultimo_id =-1\n",
    "ultima_data = datetime.datetime.now()\n",
    "\n",
    "# Tempo para atualização do ultimo_id (em minutos)\n",
    "tempo_de_reset_do_ultimo_id = 60\n",
    "\n",
    "# Função para a captura de Tweets\n",
    "def getTweets(palavra_de_procura, quantidade_de_procuras_por_api, ultimo_id, ultima_data):\n",
    "    # Lista do resultado final da captura\n",
    "    resultado = []\n",
    "    \n",
    "    # Numero da API que ta sendo usada\n",
    "    numero_api = 0\n",
    "    \n",
    "    # Captura o numero de tweets adicionados por API\n",
    "    resultado_api = {}\n",
    "    \n",
    "    if ultima_data <= datetime.datetime.now():\n",
    "        print(\"AVISO: Houve um reinicio do ultimo_id\")\n",
    "        ultima_data = datetime.datetime.now() + datetime.timedelta(minutes=tempo_de_reset_do_ultimo_id)\n",
    "        ultimo_id = -1\n",
    "    \n",
    "    # Para cada API gerada\n",
    "    for api in apis:\n",
    "        numero_api = numero_api + 1\n",
    "        \n",
    "        # Contagem de captura para a API\n",
    "        contagem = 0\n",
    "        \n",
    "        # Inicia a contagem para a API\n",
    "        resultado_api[numero_api] = 0\n",
    "        \n",
    "        if ultimo_id < 0:\n",
    "            cursor = tweepy.Cursor(api.search, q=palavra_de_procura, lang=\"pt\", tweet_mode=\"extended\")\n",
    "        else:\n",
    "            cursor = tweepy.Cursor(api.search, q=palavra_de_procura, lang=\"pt\", tweet_mode=\"extended\", max_id=ultimo_id)\n",
    "        \n",
    "        # Captura de um novo tweet\n",
    "        for tweet in cursor.items(quantidade_de_procuras_por_api):\n",
    "            \n",
    "            # Texto do tweet capturado\n",
    "            texto = tweet.full_text.lower()\n",
    "                \n",
    "            # Verfica se a palavra de procura se encontra no texto:\n",
    "            if palavra_de_procura.lower() in texto:\n",
    "                    \n",
    "                # Não adiciona tweet duplicado\n",
    "                if texto not in resultado:\n",
    "                        \n",
    "                    # Adiciona o texto ao resultado\n",
    "                    resultado.append(texto)\n",
    "                        \n",
    "                    # Adiciona as capturas da api + 1\n",
    "                    resultado_api[numero_api] = resultado_api[numero_api] + 1\n",
    "                \n",
    "            # Adiciona 1 captura a contagem de capturas para esta API\n",
    "            contagem = contagem + 1\n",
    "                \n",
    "            print(\"API {0} de {1} - {2:.0f}% das capturas realizadas, {3} tweets validos\".format(\n",
    "                  numero_api, len(apis), (contagem/quantidade_de_procuras_por_api)*100, len(resultado)), end=\"\\r\")\n",
    "            \n",
    "            # Captura o ultimo id da captura para não duplicar pedido\n",
    "            if contagem == quantidade_de_procuras_por_api:\n",
    "                ultimo_id = tweet.id\n",
    "            \n",
    "    # Embaralhando as mensagens para reduzir um possível viés\n",
    "    shuffle(resultado)\n",
    "    \n",
    "    for api, quantidade in resultado_api.items():\n",
    "        print(\"A API {0} capturou {1} tweets de {2}, equivalente a {3:.2f}% do total\".format(api, quantidade, len(resultado),\n",
    "                                                                                         quantidade/len(resultado)*100))\n",
    "    return resultado, ultimo_id, ultima_data\n",
    "\n",
    "# Função para limpar um texto de um Tweet\n",
    "def limpar_texto(texto, marca):\n",
    "    # Remove Menções\n",
    "    texto = re.sub(\"@[^ ]*\", \"\", texto)\n",
    "    # Remove links\n",
    "    texto = re.sub(\"http(|s):\\/\\/[^ \\n]*\", \"\", texto)\n",
    "    # Remove Pontuações\n",
    "    texto = re.sub(\"[\" + string.punctuation + \"]+\", \"\", texto)\n",
    "    # Remove a marca analisada\n",
    "    return re.sub(marca.lower(), \"\", texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha de uma marca e coleta das mensagens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separação da marca a ser analisada e a quantiade de capturas a ser feitas para cada api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escolha da marca a ser analisada\n",
    "marca = \"Ifood\"\n",
    "\n",
    "# Esolha de quantidade por api (no caso quantidade*2)\n",
    "quantidade_por_api = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparação para a nomeação dos Excels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de capturas ja realizadas\n",
    "numero_de_capturas_realizadas = 1\n",
    "\n",
    "# Verifica dentro do diretorio o numero de capturas realizadas\n",
    "for file in os.listdir(\".\"):\n",
    "    if marca in file:\n",
    "        numero_de_capturas_realizadas = numero_de_capturas_realizadas + 1\n",
    "\n",
    "# Cria o nome do excel a ser criado\n",
    "nome_base_do_arquivo = marca + \"-\" + str(numero_de_capturas_realizadas) \n",
    "\n",
    "nome_do_arquivo = nome_base_do_arquivo + \".xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Captura de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dados_iniciais, ultimo_id, ultima_data = getTweets(marca, quantidade_por_api, ultimo_id, ultima_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salva os dados capturados em um Execel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tabela = pd.DataFrame({\"Tweets\": pd.Series(dados_iniciais), \"Categoria\": [\"\"]*len(dados_iniciais)})\n",
    "#tabela.to_excel(nome_do_arquivo, index = False)\n",
    "\n",
    "#print(\"O Excel\", nome_do_arquivo, \"foi gerado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Classificando as mensagens na coragem\n",
    "\n",
    "Esta etapa é manual, feita pelo Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Considerando apenas as mensagens da planilha Treinamento, ensine  seu classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma os excels avaliados em um DataFrame\n",
    "\n",
    "def criar_naive_bayes(excels, marca):\n",
    "    # Cria uma tabela incial com valores inciais\n",
    "    tabela_processada = pd.DataFrame({\"Tweets\": [\"\"], \"Categoria\": [\"\"],\n",
    "                                      \"Categoria atribuida automaticamente\": [\"\"]})\n",
    "    \n",
    "    # Junta todas as tabelas em uma e garantindo que não tenha dupliacatas na junção\n",
    "    for tabela in excels:\n",
    "        tabela_processada = pd.concat([tabela_processada, pd.read_excel(tabela)], sort=False).drop_duplicates(subset=\"Tweets\")\n",
    "    \n",
    "    # Tabela contento todas as tabelas exluindo a tabela inicial\n",
    "    tabela_processada = tabela_processada.iloc[1:]\n",
    "    \n",
    "    # Variavel contendo todas as categorias selecionadas\n",
    "    categorias = set(tabela_processada[\"Categoria\"])\n",
    "    \n",
    "    # Dicionario resultado contendo a categoria como chave e a Sereis de probabilidade como valor\n",
    "    dict_resultado = {}\n",
    "    \n",
    "    for categoria in categorias:\n",
    "        # Separa os dados da tabela com o valor selecionado uma categoria\n",
    "        sub_tabela = tabela_processada[tabela_processada[\"Categoria\"] == categoria]\n",
    "        \n",
    "        # Juta todas os Tweets em uma string\n",
    "        mensagens = limpar_texto(sub_tabela[\"Tweets\"].str.cat(), marca)\n",
    "        \n",
    "        # Cria uma series contendo todas as palavras das mensagens\n",
    "        series = pd.Series(mensagens.split())\n",
    "        \n",
    "        # Cria uma series contendo a probabilidade de cada palavra estar na categoria selecionada\n",
    "        dict_resultado[categoria] = [series.value_counts(True), (len(sub_tabela.index)/len(tabela_processada.index))]\n",
    "    \n",
    "    return dict_resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Agora você deve testar o seu classificador com a base de Testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para executar um teste dando novos tweets ultilizado um dicionanrio da naive_bayes\n",
    "\n",
    "def executar_teste(novos_tweets, dict_de_atribuicoes, marca):\n",
    "    # Lista contendo os resultados obtidos automaticamente para cada tweet novo\n",
    "    resultado = []\n",
    "    \n",
    "    #Separa cada tweet nos novos tweets\n",
    "    for tweet in novos_tweets:\n",
    "        \n",
    "        # Aramazena as probabilidades de cada categoria no dicionario\n",
    "        probabilidades = {}\n",
    "        \n",
    "        # Separa cada palavra no tweet\n",
    "        for palavra in limpar_texto(tweet, marca).split():\n",
    "            \n",
    "            # Para cada tipo de categoria\n",
    "            for categoria, dados_da_probabilidade in dict_de_atribuicoes.items():\n",
    "                \n",
    "                # Probailidade de cada paalvra para esssa categoria\n",
    "                probabilidade = dados_da_probabilidade[0]\n",
    "                \n",
    "                # Verifica se a palavra se encontra em uma das probabilidades da categoria\n",
    "                if palavra in probabilidade:\n",
    "                    \n",
    "                    # Adiciona a probabilidade da palavra ao conjunto das probabilidades\n",
    "                    if categoria in probabilidades:\n",
    "                        probabilidades[categoria] = probabilidades[categoria] * probabilidade[palavra]\n",
    "                    else:\n",
    "                        probabilidades[categoria] = probabilidade[palavra] * dados_da_probabilidade[1]\n",
    "        \n",
    "        # Adiciona ao resultado o valor atribuido a esta mensagem\n",
    "        \n",
    "        # Não corespondeu a nenhuma categoria\n",
    "        if len(probabilidades) == 0:\n",
    "            resultado.append(-1)\n",
    "        else:\n",
    "            # Categoria identificado e adicionada\n",
    "            resultado.append(max(probabilidades, key=probabilidades.get))\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escolha da marca a ser analisada\n",
    "marca = \"Ifood\"\n",
    "\n",
    "# Esolha de quantidade por api (no caso quantidade*2)\n",
    "quantidade_por_api = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um dicionario para o teste do naive-bayes\n",
    "\n",
    "tabelas_avaliadas = []\n",
    "tabelas_avaliadas.append(\"Ifood-Original.xlsx\")\n",
    "tabelas_avaliadas.append(\"Ifood-Teste-2.xlsx\")\n",
    "\n",
    "dict_naive_bayes = criar_naive_bayes(tabelas_avaliadas, marca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A API 1 capturou 151 tweets de 324, equivalente a 46.60% do total\n",
      "A API 2 capturou 173 tweets de 324, equivalente a 53.40% do total\n"
     ]
    }
   ],
   "source": [
    "# Gera um novo resultado\n",
    "\n",
    "novos_tweets, ultimo_id, ultima_data = getTweets(marca, quantidade_por_api, ultimo_id, ultima_data)\n",
    "\n",
    "resultado_dos_novos_tweets = executar_teste(novos_tweets, dict_naive_bayes, marca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Excel Ifood-Teste-4.xlsx foi gerado!\n"
     ]
    }
   ],
   "source": [
    "# Numero de testes ja realizados\n",
    "numero_de_capturas_realizadas = 1\n",
    "\n",
    "# Verifica dentro do diretorio o numero de testes ja realizados\n",
    "for file in os.listdir(\".\"):\n",
    "    if marca in file:\n",
    "        numero_de_capturas_realizadas = numero_de_capturas_realizadas + 1\n",
    "\n",
    "# Cria o nome do excel a ser criado\n",
    "nome_do_arquivo_de_teste = marca + \"-Teste-\" + str(numero_de_capturas_realizadas) + \".xlsx\"\n",
    "\n",
    "tabela = pd.DataFrame({\"Tweets\": pd.Series(novos_tweets), \"Categoria\": [\"\"]*len(novos_tweets),\n",
    "                       \"Categoria atribuida automaticamente\": resultado_dos_novos_tweets})\n",
    "\n",
    "tabela.to_excel(nome_do_arquivo_de_teste, index=False)\n",
    "\n",
    "print(\"O Excel\", nome_do_arquivo_de_teste, \"foi gerado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Concluindo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperfeiçoamento:\n",
    "\n",
    "Os trabalhos vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separação de espaços entre palavras e emojis ou emojis e emojis\n",
    "* Propor outras limpezas e transformações que não afetem a qualidade da informação ou classificação\n",
    "* Criar categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que não posso usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que periodicamente realiza análise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
