{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto 2 - Ciência dos Dados - Estudo sobre Ifood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nome: Lucas Hix\n",
    "\n",
    "Nome: Andrea Mindlin Tessler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Classificador automático de sentimento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando o ambiente no jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações\n",
    "\n",
    "import datetime\n",
    "import tweepy\n",
    "import math\n",
    "import os.path\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Autenticando no  Twitter\n",
    "\n",
    "* Conta - Lucas Hix: ***@Peng1104Oficial***\n",
    "* Conta - Andrea Tessler: ***@Andrea25519779***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados de autenticação do twitter:\n",
    "\n",
    "def mkAPI(filename):\n",
    "    # leitura do arquivo no formato JSON\n",
    "    \n",
    "    with open(filename) as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    # Configurando a biblioteca. Não modificar\n",
    "    \n",
    "    auth = tweepy.OAuthHandler(data['consumer_key'], data['consumer_secret'])\n",
    "    auth.set_access_token(data['access_token'], data['access_token_secret'])\n",
    "    \n",
    "    # Cria um objeto de API\n",
    "    return tweepy.API(auth)\n",
    "\n",
    "# Conta @Peng1104Oficial\n",
    "api_lucas = mkAPI(\"lucas-auth.pass\")\n",
    "\n",
    "# Conta @Andrea25519779\n",
    "api_andrea = mkAPI(\"andrea-auth.pass\")\n",
    "\n",
    "#Lista contendo as APIs\n",
    "apis = [api_lucas, api_andrea]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Etapas do projeto:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções essenciais para o projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variaveis de suporte para a captura de Tweets\n",
    "ultimo_id =-1\n",
    "ultima_data = datetime.datetime.now()\n",
    "\n",
    "# Tempo para atualização do ultimo_id (em minutos)\n",
    "tempo_de_reset_do_ultimo_id = 60\n",
    "\n",
    "# Função para a captura de Tweets\n",
    "def getTweets(palavra_de_procura, quantidade_de_procuras_por_api, ultimo_id, ultima_data):\n",
    "    # Lista do resultado do final da captura (os tweets selecionados)\n",
    "    resultado = []\n",
    "    \n",
    "    # Numero da API que ta sendo usada (uso para debug)\n",
    "    numero_api = 0\n",
    "    \n",
    "    # Captura o numero de tweets adicionados por API\n",
    "    resultado_api = {}\n",
    "    \n",
    "    # Renicia o ultimo_id para voltar a pegar tweets recentes\n",
    "    \n",
    "    if ultima_data <= datetime.datetime.now():\n",
    "        ultima_data = datetime.datetime.now() + datetime.timedelta(minutes=tempo_de_reset_do_ultimo_id)\n",
    "        ultimo_id = -1\n",
    "        \n",
    "        print(\"AVISO: Houve um reinicio do ultimo_id, data_atual =\", ultima_data)\n",
    "    \n",
    "    # Para cada API gerada\n",
    "    for api in apis:\n",
    "        numero_api = numero_api + 1\n",
    "        \n",
    "        # Contagem de captura para a API\n",
    "        contagem = 0\n",
    "        \n",
    "        # Inicia a contagem para a API\n",
    "        resultado_api[numero_api] = 0\n",
    "        \n",
    "        if ultimo_id < 0:\n",
    "            cursor = tweepy.Cursor(api.search, q=palavra_de_procura, lang=\"pt\", tweet_mode=\"extended\")\n",
    "        else:\n",
    "            cursor = tweepy.Cursor(api.search, q=palavra_de_procura, lang=\"pt\", tweet_mode=\"extended\", max_id=ultimo_id)\n",
    "        \n",
    "        # Captura de um novo tweet\n",
    "        for tweet in cursor.items(quantidade_de_procuras_por_api):\n",
    "            \n",
    "            # Texto do tweet capturado\n",
    "            texto = tweet.full_text.lower()\n",
    "                \n",
    "            # Verfica se a palavra de procura se encontra no texto:\n",
    "            if palavra_de_procura.lower() in texto:\n",
    "                    \n",
    "                # Não adiciona tweet duplicado\n",
    "                if texto not in resultado:\n",
    "                        \n",
    "                    # Adiciona o tweet ao resultado\n",
    "                    resultado.append(texto)\n",
    "                        \n",
    "                    # Adiciona as capturas da api + 1\n",
    "                    resultado_api[numero_api] = resultado_api[numero_api] + 1\n",
    "                \n",
    "            # Adiciona 1 captura a contagem de capturas para esta API\n",
    "            contagem = contagem + 1\n",
    "            \n",
    "            # Debug para saber como esta indo o código\n",
    "            print(\"API {0} de {1} - {2:.0f}% das capturas realizadas, {3} tweets validos\".format(\n",
    "                  numero_api, len(apis), (contagem/quantidade_de_procuras_por_api)*100, len(resultado)), end=\"\\r\")\n",
    "            \n",
    "            # Captura o ultimo id da captura para não duplicar pedido\n",
    "            if contagem == quantidade_de_procuras_por_api:\n",
    "                ultimo_id = tweet.id\n",
    "            \n",
    "    # Embaralhando as mensagens para reduzir um possível viés\n",
    "    shuffle(resultado)\n",
    "    \n",
    "    # Mensagem debug\n",
    "    for api, quantidade in resultado_api.items():\n",
    "        print(\"A API {0} capturou {1} tweets de {2}, equivalente a {3:.2f}% do total\".format(api, quantidade, len(resultado), quantidade/len(resultado)*100))\n",
    "    \n",
    "    return resultado, ultimo_id, ultima_data\n",
    "\n",
    "# Função para limpar um texto de um Tweet, poder avaliar o menos sem pontuações links e menções de nomes assim como a palavra ultilizada na captura\n",
    "def limpar_texto(texto, marca):\n",
    "    # Remove Menções\n",
    "    texto = re.sub(\"@[^ ]*\", \"\", texto)\n",
    "    \n",
    "    # Remove links\n",
    "    texto = re.sub(\"http(|s):\\/\\/[^ \\n]*\", \"\", texto)\n",
    "    \n",
    "    # Remove Pontuações\n",
    "    texto = re.sub(\"[\" + string.punctuation + \"]+\", \"\", texto)\n",
    "    \n",
    "    # Remove a marca analisada\n",
    "    return re.sub(marca.lower(), \"\", texto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escolha de uma marca e coleta das mensagens pela primeira vez"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separação da marca a ser analisada e a quantiade de capturas a ser feitas para cada api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escolha da marca a ser analisada\n",
    "marca = \"Ifood\"\n",
    "\n",
    "# Esolha de quantidade por api (no caso quantidade*2)\n",
    "quantidade_por_api = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparação para a nomeação dos Excels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de capturas ja realizadas\n",
    "numero_de_capturas_realizadas = 1\n",
    "\n",
    "# Verifica dentro do diretorio o numero de capturas realizadas\n",
    "for file in os.listdir(\".\"):\n",
    "    if marca in file:\n",
    "        numero_de_capturas_realizadas = numero_de_capturas_realizadas + 1\n",
    "\n",
    "# Cria o nome do excel a ser criado\n",
    "nome_base_do_arquivo = marca + \"-\" + str(numero_de_capturas_realizadas) \n",
    "\n",
    "nome_do_arquivo = nome_base_do_arquivo + \".xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Captura dos dados iniciais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dados_iniciais, ultimo_id, ultima_data = getTweets(marca, quantidade_por_api, ultimo_id, ultima_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salva os dados iniciais capturados em um Execel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tabela = pd.DataFrame({\"Tweets\": pd.Series(dados_iniciais), \"Categoria\": [\"\"]*len(dados_iniciais)})\n",
    "#tabela.to_excel(nome_do_arquivo, index = False)\n",
    "\n",
    "#print(\"O Excel\", nome_do_arquivo, \"foi gerado!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Classificando as mensagens\n",
    "\n",
    "Esta etapa é manual, feita no Excel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Montando o Classificador Naive-Bayes\n",
    "\n",
    "Ensinando o classificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforma os excels avaliados em um Classificador\n",
    "\n",
    "def criar_naive_bayes(excels, marca):\n",
    "    # Cria o DataFrame da tabela de treinamento\n",
    "    \n",
    "    # Coluna Tweets -> Mensagens Capturadas do Twitter\n",
    "    # Coluna Categoria -> Coluna que foi avaliada manualmente, definindo a categoria do tweet\n",
    "    # Coluna Categoria atribuida automaticamente -> Pode ou não existir irelevante para o Naiv Bayes -> Ver: @1\n",
    "    # \n",
    "    # @1 -> Caso fosse ultilizado irira criar uma tendencia de \"vicio\", criando um loop de incertezas no classicador\n",
    "    \n",
    "    # DataFrame Inicial para o uso do pd.contact\n",
    "    tabela_treinamento = pd.DataFrame({\"Tweets\": [\"\"], \"Categoria\": [\"\"],\n",
    "                                      \"Categoria atribuida automaticamente\": [\"\"]})\n",
    "    \n",
    "    # Junta todas as tabelas de treinamento em uma\n",
    "    for tabela in excels:\n",
    "        tabela_treinamento = pd.concat([tabela_processada, pd.read_excel(tabela)], sort=False)\n",
    "    \n",
    "    # Remove o DataFrame incial (Agora so tem os dados avaliados)\n",
    "    tabela_treinamento = tabela_treinamento.iloc[1:]\n",
    "    \n",
    "    tamanho_original = len(tabela_treinamento)\n",
    "    \n",
    "    # Remove todas os Tweets duplicados da tabela conjuta de treinamento\n",
    "    tabela_treinamento = tabela_treinamento.drop_duplicates(subset=\"Tweets\")\n",
    "    \n",
    "    print(\"Foram removidas {0} linhas duplicadas para a formação do Naive Bayes\".format(tamanho_original-len(tabela_treinamento)))\n",
    "    \n",
    "    # Limpa todos os teewts e cria uma lista contendo todas as palavras da tabela de treinamento\n",
    "    palavras = limpar_texto(tabela_treinamento[\"Tweets\"].str.cat(), marca).split()\n",
    "    \n",
    "    # Cria a variavel de apoio do Laplace n2\n",
    "    todas_as_palavras = set(palavras)\n",
    "    \n",
    "    # Variavel contendo todas as categorias definidas\n",
    "    categorias = set(tabela_treinamento[\"Categoria\"])\n",
    "    \n",
    "    # Dicionario resultado contendo a categoria como chave e uma lista contendo os elementos para o lapas\n",
    "    dict_resultado = {}\n",
    "    \n",
    "    for categoria in categorias:\n",
    "        # Cria uma sub tabela contendo apenas os dados de uma categoria\n",
    "        tabela_categoria = tabela_treinamento[tabela_treinamento[\"Categoria\"] == categoria]\n",
    "        \n",
    "        # Juta todas os Tweets da categoria em uma string ultilizando a função limpar_texto\n",
    "        mensagens = limpar_texto(tabela_categoria[\"Tweets\"].str.cat(), marca)\n",
    "        \n",
    "        # Cria uma series contendo todas as palavras da categoria\n",
    "        series = pd.Series(mensagens.split())\n",
    "        \n",
    "        # Cria a probabilidade da palavra pertencer a categoria\n",
    "        prior = len(tabela_categoria.index)/len(tabela_treinamento.index)\n",
    "        \n",
    "        print(\"O prior da categoria {0} é de {1:.2f}%\".format(categoria, prior*100))\n",
    "        \n",
    "        dict_resultado[categoria] = [series.value_counts(), prior]\n",
    "    \n",
    "    return dict_resultado, todas_as_palavras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Verificando a performance\n",
    "\n",
    "Função para teste do naive_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para executar um teste dando novos tweets ultilizado um dicionanrio da naive_bayes\n",
    "\n",
    "def executar_teste(novos_tweets, dict_de_atribuicoes, todas_as_palavras, marca):\n",
    "    # Lista contendo os resultados obtidos automaticamente para cada tweet novo na sequencia dos mesmos\n",
    "    resultado = []\n",
    "    \n",
    "    #Separa cada tweet nos novos tweets\n",
    "    for tweet in novos_tweets:\n",
    "        \n",
    "        # Aramazena as probabilidades de cada categoria no dicionario para o tweet que esta sendo analisado\n",
    "        probabilidades = {}\n",
    "        \n",
    "        # Separa cada palavra no tweet a ser analisado\n",
    "        for palavra in limpar_texto(tweet, marca).split():\n",
    "            \n",
    "            # Para cada tipo de categoria\n",
    "            for categoria, dados_categoria in dict_de_atribuicoes.items():\n",
    "                \n",
    "                # A series desta categoria contendo a contagem de todas as palavras pertencendo a esta categoria\n",
    "                series_da_categoria = dados_categoria[0]\n",
    "                \n",
    "                # Ultiliza Laplace para calcular a probabilidade da palavra pertencer a essa categoria\n",
    "                if palavra in series_da_categoria:\n",
    "                    # Probabilidade = P(palavra|categoria) + 1 / quantiade de palavras na categoria + a quantidade de todas as palavras na tabela de treinamento\n",
    "                    probabilidade = (series_da_categoria[palavra] + 1) / (len(series_da_categoria) +  len(todas_as_palavras))\n",
    "                else:\n",
    "                    # Caso a palavra não pertence a categoria calcula a probabilidade da mesma pertencer\n",
    "                    # Probabilidade = 0 + 1 / quantiade de palavras na categoria + a quantidade de todas as palavras na tabela de treinamento\n",
    "                    probabilidade = 1 / (len(series_da_categoria) + len(todas_as_palavras))\n",
    "                \n",
    "                # Adiciona a probabilidade da palavra ao conjunto das probabilidades\n",
    "                if categoria in probabilidades:\n",
    "                    # Adiciona a probabiliade da palavra a probabilidade da categoria ja existente \n",
    "                    probabilidades[categoria] = probabilidades[categoria] * probabilidade\n",
    "                else:\n",
    "                    # Cria a primeira probabiliade da categoria que é igual a P(palavra) + P(categoria)\n",
    "                    probabilidades[categoria] = probabilidade * dados_categoria[1]\n",
    "        \n",
    "        # Adiciona ao resultado o valor atribuido a esta mensagem\n",
    "        \n",
    "        # Deu algum erro\n",
    "        if len(probabilidades) == 0:\n",
    "            resultado.append(-1)\n",
    "        else:\n",
    "            # Ultilizando o dicionario das probabilidades, procura a maior probabilidade e adicona a categoria que a represneta (o Naiv Bayes)\n",
    "            resultado.append(max(probabilidades, key=probabilidades.get))\n",
    "    \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Ultilizando as funções\n",
    "\n",
    "Testando todas as funções criadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numero de tentativas já feitas, \n",
    "Numero_de_tentativas = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(tabelas_avaliadas, ultimo_id, ultima_data, Numero_de_tentativas):\n",
    "    # Adiciona esta tentativa\n",
    "    Numero_de_tentativas = Numero_de_tentativas + 1\n",
    "    \n",
    "    # Captura novos tweets\n",
    "    novos_tweets, ultimo_id, ultima_data = getTweets(marca, quantidade_por_api, ultimo_id, ultima_data)\n",
    "\n",
    "    # Cria o classifcador ultizando todas as tabelas avalidas\n",
    "    classificador, todas_as_palavras = criar_naive_bayes(tabelas_avaliadas, marca)\n",
    "\n",
    "    # Cria a auto_avaliação feita pelo classficador (Naive Bayes)\n",
    "    auto_avaliação_dos_novos_tweets = executar_teste(novos_tweets, classificador, todas_as_palavras, marca)\n",
    "\n",
    "    # Cria o nome do excel a ser criado\n",
    "    nome_do_arquivo_de_teste = marca + \"-Teste-\" + str(Numero_de_tentativas) + \".xlsx\"\n",
    "\n",
    "    # Cria a tabela que vai ser salva no excel contendo os novo tweets um espaço para a avaliação manual (Categoria)\n",
    "    # e os valores atribuidos automaticamente pelo classifador em (Categoria atribuida automaticamente)\n",
    "    tabela = pd.DataFrame({\"Tweets\": pd.Series(novos_tweets), \"Categoria\": [\"\"]*len(novos_tweets),\n",
    "                           \"Categoria atribuida automaticamente\": auto_avaliação_dos_novos_tweets})\n",
    "\n",
    "    # Savalva a tabela no excel\n",
    "    tabela.to_excel(nome_do_arquivo_de_teste, index=False)\n",
    "\n",
    "    # Finaliza o teste\n",
    "    print(\"O Excel\", nome_do_arquivo_de_teste, \"foi gerado!\")\n",
    "    \n",
    "    return ultimo_id, ultima_data, Numero_de_tentativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AVISO: Houve um reinicio do ultimo_id, data_atual = 2019-09-17 17:09:51.917209\n",
      "A API 1 capturou 227 tweets de 422, equivalente a 53.79% do total\n",
      "A API 2 capturou 195 tweets de 422, equivalente a 46.21% do total\n",
      "Foram removidas 3 linhas duplicadas para a formação do Naive Bayes\n",
      "O prior da categoria CUPOM é de 9.64%\n",
      "O prior da categoria RUIM é de 34.01%\n",
      "O prior da categoria IMPOSSIVEL SABER é de 3.12%\n",
      "O prior da categoria IRRELEVANTE é de 18.78%\n",
      "O prior da categoria BOM é de 34.45%\n",
      "O Excel Ifood-Teste-6.xlsx foi gerado!\n"
     ]
    }
   ],
   "source": [
    "# Escolha da marca a ser analisada\n",
    "marca = \"Ifood\"\n",
    "\n",
    "# Esolha de quantidade por api (no caso quantidade*2)\n",
    "quantidade_por_api = 250\n",
    "\n",
    "# Lista das tabelas ja avaliadas\n",
    "tabelas_avaliadas = []\n",
    "\n",
    "tabelas_avaliadas.append(\"Ifood-Original.xlsx\")\n",
    "tabelas_avaliadas.append(\"Ifood-Teste-2.xlsx\")\n",
    "tabelas_avaliadas.append(\"Ifood-Teste-3.xlsx\")\n",
    "tabelas_avaliadas.append(\"Ifood-Teste-4.xlsx\")\n",
    "\n",
    "\n",
    "# Executa tudo e gera um novo execel de teste\n",
    "ultimo_id, ultima_data, Numero_de_tentativas = run_all(tabelas_avaliadas, ultimo_id, ultima_data, Numero_de_tentativas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "### Conclusões"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Após varias tabelas avaliadas acreditamos que poderia ser feito melhorias como atribuir um valor especial para palavras caracteristicas de uma categoria como a identificação de # nos twitters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aperfeiçoamento:\n",
    "\n",
    "Os trabalhos vão evoluir em conceito dependendo da quantidade de itens avançados:\n",
    "\n",
    "* Limpar: \\n, :, \", ', (, ), etc SEM remover emojis\n",
    "* Corrigir separação de espaços entre palavras e emojis ou emojis e emojis\n",
    "* Propor outras limpezas e transformações que não afetem a qualidade da informação ou classificação\n",
    "* Criar categorias intermediárias de relevância baseadas na probabilidade: ex.: muito relevante, relevante, neutro, irrelevante, muito irrelevante (3 categorias: C, mais categorias conta para B)\n",
    "* Explicar por que não posso usar o próprio classificador para gerar mais amostras de treinamento\n",
    "* Propor diferentes cenários para Naïve Bayes fora do contexto do projeto\n",
    "* Sugerir e explicar melhorias reais com indicações concretas de como implementar (indicar como fazer e indicar material de pesquisa)\n",
    "* Montar um dashboard que periodicamente realiza análise de sentimento e visualiza estes dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referências"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Naive Bayes and Text Classification](https://arxiv.org/pdf/1410.5329.pdf)  **Mais completo**\n",
    "\n",
    "[A practical explanation of a Naive Bayes Classifier](https://monkeylearn.com/blog/practical-explanation-naive-bayes-classifier/) **Mais simples**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
